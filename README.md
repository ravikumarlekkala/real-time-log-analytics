# ğŸ“Š Real-Time Log Analytics System

## ğŸš€ Overview

The **Real-Time Log Analytics System** is a scalable platform for **real-time log ingestion, processing and analytics**.  
It seamlessly integrates **streaming data from Kafka** and **historical data from MongoDB**, processes it using **Apache Spark Structured Streaming**, and persists enriched insights into **PostgreSQL** for analytics, dashboards, and alerting.

### âœ¨ Key Capabilities
- â±ï¸ **Real-time monitoring** of distributed application logs  
- ğŸš¨ **Error detection & alerting** with configurable thresholds  
- âš¡ **Performance insights**: request latency, throughput, error rates  
- ğŸ“Š **Interactive dashboards** with **Grafana / Superset / Atlas Charts**  
- ğŸ”„ **Hybrid pipeline**: combine real-time (Kafka) + historical (MongoDB)  

---

## ğŸ—ï¸ System Architecture

```text
Kafka (Streaming Logs) ----\
                            >---- Spark Structured Streaming ----> PostgreSQL (Analytics DB) ----> Grafana ----> Alerts
MongoDB (Historical Logs) --/                      
````

![Architecture](Asserts/Images/dashboard1.png)

---

## ğŸ“‚ Project Structure

```text
src/main/scala/com/loganalytics/
  â”œâ”€â”€ controller/
  â”‚    â”œâ”€â”€ KafkaStreamController.scala   # Kafka streaming pipeline
  â”‚    â”œâ”€â”€ MongoBatchController.scala    # Mongo batch pipeline
  â”‚    â””â”€â”€ LogStreamController.scala     # Orchestrates both pipelines
  â”œâ”€â”€ dao/
  â”‚    â”œâ”€â”€ KafkaDAO.scala                # Kafka ingestion
  â”‚    â”œâ”€â”€ MongoDAO.scala                # Mongo ingestion
  â”‚    â””â”€â”€ PostgresDAO.scala             # PostgreSQL persistence
  â”œâ”€â”€ service/
  â”‚    â”œâ”€â”€ LogParserService.scala        # Parse JSON logs â†’ structured DF
  â”‚    â”œâ”€â”€ RawLogService.scala           # Normalize raw schema
  â”‚    â”œâ”€â”€ MongoLogService.scala         # Transform MongoDB logs
  â”‚    â”œâ”€â”€ AlertingService.scala         # Detect error/latency alerts
  â”‚    â””â”€â”€ AggregationService.scala      # Windowed aggregations
  â””â”€â”€ utils/
       â””â”€â”€ SchemaUtils.scala             # Centralized schema definitions
```

---

## âš™ï¸ Tech Stack

| Technology                                                                                      | Role                         |
| ----------------------------------------------------------------------------------------------- | ---------------------------- |
| ![Kafka](https://img.shields.io/badge/Apache%20Kafka-231F20?logo=apache-kafka\&logoColor=white) | Real-time log ingestion      |
| ![MongoDB](https://img.shields.io/badge/MongoDB-4EA94B?logo=mongodb\&logoColor=white)           | Historical logs + metadata   |
| ![Spark](https://img.shields.io/badge/Apache%20Spark-E25A1C?logo=apachespark\&logoColor=white)  | ETL, enrichment & analytics  |
| ![PostgreSQL](https://img.shields.io/badge/PostgreSQL-316192?logo=postgresql\&logoColor=white)  | Analytics-ready data sink    |
| ![Scala](https://img.shields.io/badge/Scala-DC322F?logo=scala\&logoColor=white)                 | Data pipeline implementation |
| ![Docker](https://img.shields.io/badge/Docker-2496ED?logo=docker\&logoColor=white)              | Containerized deployment     |
| ![Grafana](https://img.shields.io/badge/Grafana-F46800?logo=grafana\&logoColor=white)           | Visualization & alerting     |

---

## ğŸ“¥ Setup & Installation

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/your-username/real-time-log-analytics-system.git
cd real-time-log-analytics-system
```

### 2ï¸âƒ£ Configure Connections

Update credentials in:

```
src/main/resources/application.conf
```

### 3ï¸âƒ£ Start Dependencies

* â–¶ï¸ Start **Kafka broker + Zookeeper**
* ğŸ˜ Start **PostgreSQL** instance
* â˜ï¸ Ensure **MongoDB Atlas** cluster is accessible

### 4ï¸âƒ£ Build & Run Pipelines

```bash
# Start Zookeeper
bin/zookeeper-server-start.sh config/zookeeper.properties  

# Start Kafka broker
bin/kafka-server-start.sh config/server.properties  

# Start consumer (debugging)
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic logs --from-beginning
```

### 5ï¸âƒ£ Run Spark Job

```bash
sbt run
```

---

## ğŸ“Š Visualization

### Grafana

* Connect PostgreSQL as a data source
* Import prebuilt **log monitoring dashboards**

![Grafana Dashboard](Asserts/Images/db1.jpg)

### PostgreSQL (PgAdmin)

* Explore normalized log data in **analytics-ready tables**

![PgAdmin Dashboard](Asserts/Images/pgdb.png)

### MongoDB Atlas

* Query historical logs

![MongoDB Atlas](Asserts/Images/mdb.png)

---

## ğŸ” Example Queries

**Logs per service (hourly aggregation):**

```mongodb
[{
  $group: {
    _id: { service: "$service", hour: { $hour: "$event_time" } },
    count: { $sum: 1 }
  }
}]
```

**Error trend analysis:**

```mongodb
[{
  $match: { level: "ERROR" }
}, {
  $group: {
    _id: { service: "$service", hour: { $hour: "$event_time" } },
    errors: { $sum: 1 }
  }
}]
```

---

## ğŸ–¥ï¸ Console Output

When running, youâ€™ll see Spark Structured Streaming processing batches like:

```
[KafkaController] Starting Kafka streaming pipelineâ€¦
[JdbcWriter][kafka] Processing RAW batch 12 with 105 records â†’ raw_logs
[JdbcWriter][kafka] âœ… Wrote RAW batch 12 to raw_logs
[JdbcWriter][kafka] Processing AGGREGATES batch 5 with 7 records â†’ aggregated_logs
[JdbcWriter][kafka] âœ… Wrote AGGREGATES batch 5 to aggregated_logs
[JdbcWriter][kafka] Processing ALERTS batch 4 with 2 records â†’ alerts
[JdbcWriter][kafka] âœ… Wrote ALERTS batch 4 to alerts
```

This confirms **data is being ingested, aggregated, and stored** in PostgreSQL.

---

## ğŸš¨ Alerts

Rules can be configured for anomaly detection:

* âŒ **Error rate > 5% in 10 min** â†’ Trigger alert
* ğŸ•‘ **Latency > 2s** for critical services â†’ Notify via Slack/Email

![Alert Example](Asserts/Images/alert.png)

---

## ğŸ¤ Contributing

Contributions are welcome!

1. ğŸ´ Fork the repository
2. ğŸŒ± Create a feature branch (`git checkout -b feature/new-feature`)
3. âœ… Commit changes (`git commit -m 'Add new feature'`)
4. ğŸš€ Push (`git push origin feature/new-feature`)
5. ğŸ” Open a Pull Request

---

## ğŸ“œ License

MIT License Â© 2025
